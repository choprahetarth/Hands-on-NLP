{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtW1ydtW86J0DZEgHSyPv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choprahetarth/Hands-on-NLP/blob/main/NLTK_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvmjLyJqDAVS",
        "outputId": "a550b94f-4428-4b88-c83a-69fa51c854c8"
      },
      "source": [
        "import this"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Zen of Python, by Tim Peters\n",
            "\n",
            "Beautiful is better than ugly.\n",
            "Explicit is better than implicit.\n",
            "Simple is better than complex.\n",
            "Complex is better than complicated.\n",
            "Flat is better than nested.\n",
            "Sparse is better than dense.\n",
            "Readability counts.\n",
            "Special cases aren't special enough to break the rules.\n",
            "Although practicality beats purity.\n",
            "Errors should never pass silently.\n",
            "Unless explicitly silenced.\n",
            "In the face of ambiguity, refuse the temptation to guess.\n",
            "There should be one-- and preferably only one --obvious way to do it.\n",
            "Although that way may not be obvious at first unless you're Dutch.\n",
            "Now is better than never.\n",
            "Although never is often better than *right* now.\n",
            "If the implementation is hard to explain, it's a bad idea.\n",
            "If the implementation is easy to explain, it may be a good idea.\n",
            "Namespaces are one honking great idea -- let's do more of those!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBrLV14hA5K8"
      },
      "source": [
        "# My Experiments with NLTK and similar libraries such as Spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbS1MIeNA9o3"
      },
      "source": [
        "Following document is basically my learnings and experiments related to NLTK, and could be a quick followthrough while prepping for technical interviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luU3KIR6BjFm"
      },
      "source": [
        "1. Download NLTK all libraries and do tokenization for a paragraph[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwFnYMxY9-11",
        "outputId": "7083d201-7bf3-44cf-fcde-088742c2f2fa"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iswXM81MBPRj",
        "outputId": "53d42f19-8dae-4b6b-ba77-9f4c945ccbbf"
      },
      "source": [
        "#command used to download the sentence tokenizer\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwXglKYqCm3u",
        "outputId": "cfd39785-65fb-432b-c3a1-324a20feb80a"
      },
      "source": [
        "# This paragraph is taken from Google's Transformer paper\n",
        "paragraph = \"\"\"The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including\n",
        "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
        "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
        "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
        "best models from the literature. We show that the Transformer generalizes well to\n",
        "other tasks by applying it successfully to English constituency parsing both with\n",
        "large and limited training data.\"\"\"\n",
        "# Get total count of char's inside the paragraph\n",
        "print(len(paragraph))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSIolI4vDRIH"
      },
      "source": [
        "# Apply Sentence Tokenization which segregates a paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_lnX69Dit3",
        "outputId": "0dfce7de-b04c-4c65-9a39-61da2573b697"
      },
      "source": [
        "print(sentences)\n",
        "print(len(sentences))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder.', 'The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism.', 'We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely.', 'Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train.', 'Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU.', 'On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.', 'We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.']\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_KzDlmcDpBs"
      },
      "source": [
        "# Apply Word Tokenization which segregates sentences into words\n",
        "words = nltk.word_tokenize(paragraph)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKn4bv9HEDq9",
        "outputId": "e133bde4-d372-4c6c-a0c7-886dd8e43a80"
      },
      "source": [
        "print(words)\n",
        "print(len(words))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'dominant', 'sequence', 'transduction', 'models', 'are', 'based', 'on', 'complex', 'recurrent', 'or', 'convolutional', 'neural', 'networks', 'that', 'include', 'an', 'encoder', 'and', 'a', 'decoder', '.', 'The', 'best', 'performing', 'models', 'also', 'connect', 'the', 'encoder', 'and', 'decoder', 'through', 'an', 'attention', 'mechanism', '.', 'We', 'propose', 'a', 'new', 'simple', 'network', 'architecture', ',', 'the', 'Transformer', ',', 'based', 'solely', 'on', 'attention', 'mechanisms', ',', 'dispensing', 'with', 'recurrence', 'and', 'convolutions', 'entirely', '.', 'Experiments', 'on', 'two', 'machine', 'translation', 'tasks', 'show', 'these', 'models', 'to', 'be', 'superior', 'in', 'quality', 'while', 'being', 'more', 'parallelizable', 'and', 'requiring', 'significantly', 'less', 'time', 'to', 'train', '.', 'Our', 'model', 'achieves', '28.4', 'BLEU', 'on', 'the', 'WMT', '2014', 'Englishto-German', 'translation', 'task', ',', 'improving', 'over', 'the', 'existing', 'best', 'results', ',', 'including', 'ensembles', ',', 'by', 'over', '2', 'BLEU', '.', 'On', 'the', 'WMT', '2014', 'English-to-French', 'translation', 'task', ',', 'our', 'model', 'establishes', 'a', 'new', 'single-model', 'state-of-the-art', 'BLEU', 'score', 'of', '41.8', 'after', 'training', 'for', '3.5', 'days', 'on', 'eight', 'GPUs', ',', 'a', 'small', 'fraction', 'of', 'the', 'training', 'costs', 'of', 'the', 'best', 'models', 'from', 'the', 'literature', '.', 'We', 'show', 'that', 'the', 'Transformer', 'generalizes', 'well', 'to', 'other', 'tasks', 'by', 'applying', 'it', 'successfully', 'to', 'English', 'constituency', 'parsing', 'both', 'with', 'large', 'and', 'limited', 'training', 'data', '.']\n",
            "184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeSc5t34GrA0"
      },
      "source": [
        "2. Start Stemming and Lemmatization using NLTK and spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG6CGZsdEFKy"
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-LEhvAkdBqk"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao0oNw9OfDP1"
      },
      "source": [
        "stopwords = stopwords.words('english')\n",
        "stopwords = (set(stopwords))\n",
        "stemmedSentences = sentences\n",
        "lemmatizedSentence = sentences"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_m2RaudFZk"
      },
      "source": [
        "for i in range(len(sentences)):\n",
        "  nonStopList = []\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  nonStopList = list(filter(lambda x: (x not in stopwords),words))\n",
        "  stemmedList = [stemmer.stem(x) for x in nonStopList]\n",
        "  #words = [stemmer.stem[singleWord] for singleWord in words if singleWord not in stopwords]\n",
        "  stemmedSentences[i] = ' '.join(stemmedList)\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  nonStopList = []\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  nonStopList = list(filter(lambda x: (x not in stopwords),words))\n",
        "  lemmatizedList = [lemmatizer.lemmatize(x) for x in nonStopList]\n",
        "  #words = [stemmer.stem[singleWord] for singleWord in words if singleWord not in stopwords]\n",
        "  lemmatizedSentence[i] = ' '.join(lemmatizedList)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVnh-ra_eor1",
        "outputId": "5bf888e4-e5d5-4e8d-d1c4-715e3fb2224d"
      },
      "source": [
        "stemmedSentences"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['domin sequenc transduct model base complex recurr convolut neural network includ encod decod .',\n",
              " 'best perform model also connect encod decod attent mechan .',\n",
              " 'We propos new simpl network architectur , transform , base sole attent mechan , dispens recurr convolut entir .',\n",
              " 'experi two machin translat task show model superior qualiti paralleliz requir significantli le time train .',\n",
              " 'model achiev 28.4 bleu wmt 2014 englishto-german translat task , improv exist best result , includ ensembl , 2 bleu .',\n",
              " 'On wmt 2014 english-to-french translat task , model establish new single-model state-of-the-art bleu score 41.8 train 3.5 day eight gpu , small fraction train cost best model literatur .',\n",
              " 'We show transform gener well task appli success english constitu par larg limit train data .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsuV7baDzLXR",
        "outputId": "efe300c7-3b51-43c6-905e-cab047469d2b"
      },
      "source": [
        "lemmatizedSentence"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['domin sequenc transduct model base complex recurr convolut neural network includ encod decod .',\n",
              " 'best perform model also connect encod decod attent mechan .',\n",
              " 'We propo new simpl network architectur , transform , base sole attent mechan , dispen recurr convolut entir .',\n",
              " 'experi two machin translat task show model superior qualiti paralleliz requir significantli le time train .',\n",
              " 'model achiev 28.4 bleu wmt 2014 englishto-german translat task , improv exist best result , includ ensembl , 2 bleu .',\n",
              " 'On wmt 2014 english-to-french translat task , model establish new single-model state-of-the-art bleu score 41.8 train 3.5 day eight gpu , small fraction train cost best model literatur .',\n",
              " 'We show transform gener well task appli success english constitu par larg limit train data .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H0qdrg12Bd7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}