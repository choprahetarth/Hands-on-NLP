{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hetarthchopra/bert-from-huggingface?scriptVersionId=112834644\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"This code is heavily inspired from the code of https://www.kaggle.com/code/ksork6s4/uspppm-bert-for-patents-baseline-train. I am simply using it for my own learning of HuggingFace","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-03T11:15:26.2164Z","iopub.execute_input":"2022-12-03T11:15:26.217094Z","iopub.status.idle":"2022-12-03T11:15:26.242305Z","shell.execute_reply.started":"2022-12-03T11:15:26.21706Z","shell.execute_reply":"2022-12-03T11:15:26.241216Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/kaggle/input/cpc-codes/titles.csv\n/kaggle/input/deberta-v3-large/deberta-v3-large/spm.model\n/kaggle/input/deberta-v3-large/deberta-v3-large/config.json\n/kaggle/input/deberta-v3-large/deberta-v3-large/README.md\n/kaggle/input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\n/kaggle/input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\n/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np \nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport shutil\n\nfrom torch.utils.data import DataLoader, Dataset\nimport datasets, transformers\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:26.492926Z","iopub.execute_input":"2022-12-03T11:15:26.493262Z","iopub.status.idle":"2022-12-03T11:15:26.499002Z","shell.execute_reply.started":"2022-12-03T11:15:26.493234Z","shell.execute_reply":"2022-12-03T11:15:26.497894Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    input_path = '../input/us-patent-phrase-to-phrase-matching/'\n    model_path = '../input/deberta-v3-large/deberta-v3-large/'\n    \n    learning_rate = 2e-5\n    weight_decay = 0.01\n    num_fold = 5\n    epochs = 5\n    batch_size = 8\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:26.615924Z","iopub.execute_input":"2022-12-03T11:15:26.616479Z","iopub.status.idle":"2022-12-03T11:15:26.622412Z","shell.execute_reply.started":"2022-12-03T11:15:26.616443Z","shell.execute_reply":"2022-12-03T11:15:26.621227Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## PreProcessing","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{CFG.input_path}train.csv\")\ntitles = pd.read_csv('../input/cpc-codes/titles.csv')\ntest_df = pd.read_csv(f\"{CFG.input_path}test.csv\")\ntrain_df = train_df.merge(titles, left_on='context', right_on='code') # basically this replaces \ntest_df = test_df.merge(titles, left_on='context', right_on='code') # basically this replaces ","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:26.849947Z","iopub.execute_input":"2022-12-03T11:15:26.850226Z","iopub.status.idle":"2022-12-03T11:15:27.849456Z","shell.execute_reply.started":"2022-12-03T11:15:26.850201Z","shell.execute_reply":"2022-12-03T11:15:27.84826Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# train_df = train_df.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:27.85125Z","iopub.execute_input":"2022-12-03T11:15:27.851619Z","iopub.status.idle":"2022-12-03T11:15:27.86081Z","shell.execute_reply.started":"2022-12-03T11:15:27.85159Z","shell.execute_reply":"2022-12-03T11:15:27.856585Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"fold\"] = -1\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(data[\"score\"], bins=5, labels=False) # \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'fold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:27.862419Z","iopub.execute_input":"2022-12-03T11:15:27.863144Z","iopub.status.idle":"2022-12-03T11:15:27.871772Z","shell.execute_reply.started":"2022-12-03T11:15:27.863104Z","shell.execute_reply":"2022-12-03T11:15:27.870447Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train_df['input'] = train_df['title']+'[SEP]'+train_df['anchor']\ntrain_df = create_folds(train_df, CFG.num_fold)\ntest_df['input']=test_df['title']+'[SEP]'+train_df['anchor']","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:27.875068Z","iopub.execute_input":"2022-12-03T11:15:27.875608Z","iopub.status.idle":"2022-12-03T11:15:27.907207Z","shell.execute_reply.started":"2022-12-03T11:15:27.875572Z","shell.execute_reply":"2022-12-03T11:15:27.906272Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df\n# in this label = score \n# sentence 1 = input (context+anchor)\n# sentence 2 = ","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:27.908533Z","iopub.execute_input":"2022-12-03T11:15:27.909154Z","iopub.status.idle":"2022-12-03T11:15:27.93611Z","shell.execute_reply.started":"2022-12-03T11:15:27.909114Z","shell.execute_reply":"2022-12-03T11:15:27.93509Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                  id            anchor                  target context  score  \\\n0   37d61fd2272659b1         abatement  abatement of pollution     A47   0.50   \n1   7b9652b17b68b7a4         abatement          act of abating     A47   0.75   \n2   36d72442aefd8232         abatement         active catalyst     A47   0.25   \n3   5296b0c19e1ce60e         abatement     eliminating process     A47   0.50   \n4   54c1e3b9184cb5b6         abatement           forest region     A47   0.00   \n..               ...               ...                     ...     ...    ...   \n95  a28320e15e1aa1de  cervical support         comfort support     A47   0.50   \n96  999d1bb85a8c63c7  cervical support      comfort to comfort     A47   0.25   \n97  c3f9606db5901c42  cervical support    comfort when comfort     A47   0.25   \n98  8a1215a697f793f6  cervical support              contouring     A47   0.25   \n99  10a64a8d32e76343  cervical support           foot supports     A47   0.25   \n\n   code                                              title section  class  \\\n0   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n1   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n2   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n3   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n4   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n..  ...                                                ...     ...    ...   \n95  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n96  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n97  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n98  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n99  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n\n   subclass  group  main_group  \\\n0       NaN    NaN         NaN   \n1       NaN    NaN         NaN   \n2       NaN    NaN         NaN   \n3       NaN    NaN         NaN   \n4       NaN    NaN         NaN   \n..      ...    ...         ...   \n95      NaN    NaN         NaN   \n96      NaN    NaN         NaN   \n97      NaN    NaN         NaN   \n98      NaN    NaN         NaN   \n99      NaN    NaN         NaN   \n\n                                                input  fold  \n0   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n1   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n2   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n3   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  \n4   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  \n..                                                ...   ...  \n95  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n96  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n97  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n98  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  \n99  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  \n\n[100 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>code</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n      <th>input</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>a28320e15e1aa1de</td>\n      <td>cervical support</td>\n      <td>comfort support</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>999d1bb85a8c63c7</td>\n      <td>cervical support</td>\n      <td>comfort to comfort</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>c3f9606db5901c42</td>\n      <td>cervical support</td>\n      <td>comfort when comfort</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>8a1215a697f793f6</td>\n      <td>cervical support</td>\n      <td>contouring</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>10a64a8d32e76343</td>\n      <td>cervical support</td>\n      <td>foot supports</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:27.937757Z","iopub.execute_input":"2022-12-03T11:15:27.938136Z","iopub.status.idle":"2022-12-03T11:15:29.388206Z","shell.execute_reply.started":"2022-12-03T11:15:27.938099Z","shell.execute_reply":"2022-12-03T11:15:29.386998Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"loading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nDidn't find file ../input/deberta-v3-large/deberta-v3-large/tokenizer.json. We won't load it.\nDidn't find file ../input/deberta-v3-large/deberta-v3-large/added_tokens.json. We won't load it.\nDidn't find file ../input/deberta-v3-large/deberta-v3-large/special_tokens_map.json. We won't load it.\nloading file ../input/deberta-v3-large/deberta-v3-large/spm.model\nloading file None\nloading file None\nloading file None\nloading file ../input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\nloading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TrainDataset(Dataset): # takes in the Dataset module from PyTorch \n    def __init__(self,df): # init, used to structure the dataset\n        self.inputs = df['input'].values.astype(str) # input\n        self.targets = df['target'].values.astype(str) # target\n        self.label = df['score'].values # similarity\n        \n    def __len__(self):\n        return len(self.inputs) # return length \n    \n    def __getitem__(self,item):\n        inputs = self.inputs[item] # get item mean, index dalo, and returns the value\n        targets = self.targets[item] # get target is similar, index dalo, and return the value \n        label = self.label[item] \n        return {**tokenizer(inputs,targets), 'label':label.astype(np.float32)}","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:29.391709Z","iopub.execute_input":"2022-12-03T11:15:29.392202Z","iopub.status.idle":"2022-12-03T11:15:29.399885Z","shell.execute_reply.started":"2022-12-03T11:15:29.392163Z","shell.execute_reply":"2022-12-03T11:15:29.398567Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class InferDataset(Dataset):\n    def __init__(self,df):\n        self.inputs = df['input'].values.astype(str)\n        self.targets = df['target'].values.astype(str)\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self,item):\n        inputs = self.inputs[item]\n        targets = self.targets[item]\n        return {**tokenizer(inputs,targets)}\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:29.401524Z","iopub.execute_input":"2022-12-03T11:15:29.401925Z","iopub.status.idle":"2022-12-03T11:15:29.41223Z","shell.execute_reply.started":"2022-12-03T11:15:29.40189Z","shell.execute_reply":"2022-12-03T11:15:29.411097Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.reshape(len(predictions))\n    return{'pearson':np.corrcoef(predictions,labels)[0][1]} # yaha wala pearson","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:29.414549Z","iopub.execute_input":"2022-12-03T11:15:29.415374Z","iopub.status.idle":"2022-12-03T11:15:29.423039Z","shell.execute_reply.started":"2022-12-03T11:15:29.415273Z","shell.execute_reply":"2022-12-03T11:15:29.422081Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.DataFrame()\nfor fold in range(CFG.num_fold):\n    training_data = train_df[train_df['fold']!=fold].reset_index(drop=True)\n    validation_data = train_df[train_df['fold']==fold].reset_index(drop=True)\n\n    tr_data = TrainDataset(training_data)\n    va_data = TrainDataset(validation_data)\n\n    # provide training arguments \n    args = TrainingArguments(\n        output_dir=f\"/tmp/uspppm\",\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        learning_rate = CFG.learning_rate,\n        per_device_train_batch_size=CFG.batch_size,\n        per_device_eval_batch_size=CFG.batch_size,\n        num_train_epochs=CFG.epochs,\n        weight_decay=CFG.weight_decay,\n        metric_for_best_model=\"pearson\", # is the same as yaha wala pearson\n        load_best_model_at_end=True,\n        logging_steps=10,\n    )\n    \n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path,num_labels=1)\n\n    trainer=Trainer(\n        model,\n        args,\n        train_dataset = tr_data,\n        eval_dataset = va_data,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics)\n    \n    trainer.train()\n    shutil.rmtree(f'/tmp/uspppm')\n    trainer.save_model(f'uspppm_{fold}')\n    \n    outputs = trainer.predict(va_data)\n    predictions = outputs.predictions.reshape(-1)\n    validation_data['preds'] = predictions\n    oof_df = pd.concat([oof_df, validation_data])","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:15:29.424695Z","iopub.execute_input":"2022-12-03T11:15:29.425127Z","iopub.status.idle":"2022-12-03T11:18:16.965957Z","shell.execute_reply.started":"2022-12-03T11:15:29.425093Z","shell.execute_reply":"2022-12-03T11:18:16.965065Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nloading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ../input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\nSome weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large/ were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifer.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large/ and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 50\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14/14 01:03, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.057730</td>\n      <td>0.238084</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.077300</td>\n      <td>0.054820</td>\n      <td>0.495769</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 50\n  Batch size = 8\nSaving model checkpoint to /tmp/uspppm/checkpoint-7\nConfiguration saved in /tmp/uspppm/checkpoint-7/config.json\nModel weights saved in /tmp/uspppm/checkpoint-7/pytorch_model.bin\ntokenizer config file saved in /tmp/uspppm/checkpoint-7/tokenizer_config.json\nSpecial tokens file saved in /tmp/uspppm/checkpoint-7/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 50\n  Batch size = 8\nSaving model checkpoint to /tmp/uspppm/checkpoint-14\nConfiguration saved in /tmp/uspppm/checkpoint-14/config.json\nModel weights saved in /tmp/uspppm/checkpoint-14/pytorch_model.bin\ntokenizer config file saved in /tmp/uspppm/checkpoint-14/tokenizer_config.json\nSpecial tokens file saved in /tmp/uspppm/checkpoint-14/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /tmp/uspppm/checkpoint-14 (score: 0.49576867804696334).\nSaving model checkpoint to uspppm_0\nConfiguration saved in uspppm_0/config.json\nModel weights saved in uspppm_0/pytorch_model.bin\ntokenizer config file saved in uspppm_0/tokenizer_config.json\nSpecial tokens file saved in uspppm_0/special_tokens_map.json\n***** Running Prediction *****\n  Num examples = 50\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nloading configuration file ../input/deberta-v3-large/deberta-v3-large/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"../input/deberta-v3-large/deberta-v3-large/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file ../input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\nSome weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large/ were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifer.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../input/deberta-v3-large/deberta-v3-large/ and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 50\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14/14 01:04, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.065063</td>\n      <td>0.301351</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.125000</td>\n      <td>0.059070</td>\n      <td>0.316948</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 50\n  Batch size = 8\nSaving model checkpoint to /tmp/uspppm/checkpoint-7\nConfiguration saved in /tmp/uspppm/checkpoint-7/config.json\nModel weights saved in /tmp/uspppm/checkpoint-7/pytorch_model.bin\ntokenizer config file saved in /tmp/uspppm/checkpoint-7/tokenizer_config.json\nSpecial tokens file saved in /tmp/uspppm/checkpoint-7/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 50\n  Batch size = 8\nSaving model checkpoint to /tmp/uspppm/checkpoint-14\nConfiguration saved in /tmp/uspppm/checkpoint-14/config.json\nModel weights saved in /tmp/uspppm/checkpoint-14/pytorch_model.bin\ntokenizer config file saved in /tmp/uspppm/checkpoint-14/tokenizer_config.json\nSpecial tokens file saved in /tmp/uspppm/checkpoint-14/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /tmp/uspppm/checkpoint-14 (score: 0.3169481009467462).\nSaving model checkpoint to uspppm_1\nConfiguration saved in uspppm_1/config.json\nModel weights saved in uspppm_1/pytorch_model.bin\ntokenizer config file saved in uspppm_1/tokenizer_config.json\nSpecial tokens file saved in uspppm_1/special_tokens_map.json\n***** Running Prediction *****\n  Num examples = 50\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:00]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"predictions = oof_df['preds'].values\nlabel = oof_df['score'].values\neval_pred = predictions, label\ncompute_metrics(eval_pred)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:18:16.968592Z","iopub.execute_input":"2022-12-03T11:18:16.969261Z","iopub.status.idle":"2022-12-03T11:18:16.981325Z","shell.execute_reply.started":"2022-12-03T11:18:16.969224Z","shell.execute_reply":"2022-12-03T11:18:16.980276Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'pearson': 0.07931120821210114}"},"metadata":{}}]},{"cell_type":"code","source":"oof_df","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:18:16.983091Z","iopub.execute_input":"2022-12-03T11:18:16.98391Z","iopub.status.idle":"2022-12-03T11:18:18.214648Z","shell.execute_reply.started":"2022-12-03T11:18:16.983855Z","shell.execute_reply":"2022-12-03T11:18:18.213543Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                  id            anchor                 target context  score  \\\n0   5296b0c19e1ce60e         abatement    eliminating process     A47   0.50   \n1   54c1e3b9184cb5b6         abatement          forest region     A47   0.00   \n2   0a425937a3e86d10         abatement      minimising sounds     A47   0.50   \n3   ef2d4c2e6bbb208d         abatement  mixing core materials     A47   0.25   \n4   84261a11e5d1b68b         abatement        noise reduction     A47   0.50   \n..               ...               ...                    ...     ...    ...   \n45  67340ec1e330d088  cervical support          collar design     A47   0.25   \n46  a28320e15e1aa1de  cervical support        comfort support     A47   0.50   \n47  999d1bb85a8c63c7  cervical support     comfort to comfort     A47   0.25   \n48  c3f9606db5901c42  cervical support   comfort when comfort     A47   0.25   \n49  10a64a8d32e76343  cervical support          foot supports     A47   0.25   \n\n   code                                              title section  class  \\\n0   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n1   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n2   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n3   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n4   A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n..  ...                                                ...     ...    ...   \n45  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n46  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n47  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n48  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n49  A47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0   \n\n   subclass  group  main_group  \\\n0       NaN    NaN         NaN   \n1       NaN    NaN         NaN   \n2       NaN    NaN         NaN   \n3       NaN    NaN         NaN   \n4       NaN    NaN         NaN   \n..      ...    ...         ...   \n45      NaN    NaN         NaN   \n46      NaN    NaN         NaN   \n47      NaN    NaN         NaN   \n48      NaN    NaN         NaN   \n49      NaN    NaN         NaN   \n\n                                                input  fold     preds  \n0   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  0.396115  \n1   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  0.395465  \n2   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  0.395006  \n3   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  0.395401  \n4   FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     0  0.397304  \n..                                                ...   ...       ...  \n45  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  0.357637  \n46  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  0.359003  \n47  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  0.363099  \n48  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  0.355210  \n49  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     1  0.355030  \n\n[100 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>code</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n      <th>input</th>\n      <th>fold</th>\n      <th>preds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n      <td>0.396115</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n      <td>0.395465</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0a425937a3e86d10</td>\n      <td>abatement</td>\n      <td>minimising sounds</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n      <td>0.395006</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ef2d4c2e6bbb208d</td>\n      <td>abatement</td>\n      <td>mixing core materials</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n      <td>0.395401</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84261a11e5d1b68b</td>\n      <td>abatement</td>\n      <td>noise reduction</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>0</td>\n      <td>0.397304</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>67340ec1e330d088</td>\n      <td>cervical support</td>\n      <td>collar design</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n      <td>0.357637</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>a28320e15e1aa1de</td>\n      <td>cervical support</td>\n      <td>comfort support</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n      <td>0.359003</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>999d1bb85a8c63c7</td>\n      <td>cervical support</td>\n      <td>comfort to comfort</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n      <td>0.363099</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>c3f9606db5901c42</td>\n      <td>cervical support</td>\n      <td>comfort when comfort</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n      <td>0.355210</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>10a64a8d32e76343</td>\n      <td>cervical support</td>\n      <td>foot supports</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>A47</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>1</td>\n      <td>0.355030</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"oof_df.to_csv('oof_df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:18:18.219655Z","iopub.execute_input":"2022-12-03T11:18:18.219964Z","iopub.status.idle":"2022-12-03T11:18:20.331159Z","shell.execute_reply.started":"2022-12-03T11:18:18.219925Z","shell.execute_reply":"2022-12-03T11:18:20.330188Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor fold in range(CFG.num_fold):\n    te_dataset = InferDataset(test_df)\n    model = AutoModelForSequenceClassification.from_pretrained(f'uspppm_{fold}', num_labels=1)\n    trainer = Trainer(model,tokenizer=tokenizer)\n\n    outputs = trainer.predict(te_dataset)\n    prediction = outputs.predictions.reshape(-1)\n    predictions.append(prediction)\n    \npredictions = np.mean(predictions, axis=0)\nsubmission = datasets.Dataset.from_dict({\n    'id': test_df['id'],\n    'score': predictions,\n})","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:24.043251Z","iopub.execute_input":"2022-12-03T11:22:24.043625Z","iopub.status.idle":"2022-12-03T11:22:37.885354Z","shell.execute_reply.started":"2022-12-03T11:22:24.043594Z","shell.execute_reply":"2022-12-03T11:22:37.884419Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"loading configuration file uspppm_0/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"uspppm_0\",\n  \"architectures\": [\n    \"DebertaV2ForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file uspppm_0/pytorch_model.bin\nAll model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n\nAll the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at uspppm_0.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\nNo `TrainingArguments` passed, using `output_dir=tmp_trainer`.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n***** Running Prediction *****\n  Num examples = 36\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file uspppm_1/config.json\nModel config DebertaV2Config {\n  \"_name_or_path\": \"uspppm_1\",\n  \"architectures\": [\n    \"DebertaV2ForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 1024,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file uspppm_1/pytorch_model.bin\nAll model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n\nAll the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at uspppm_1.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\nNo `TrainingArguments` passed, using `output_dir=tmp_trainer`.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n***** Running Prediction *****\n  Num examples = 36\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:00]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"submission = submission.to_pandas()\nsubmission.score = submission.score.astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:37.887721Z","iopub.execute_input":"2022-12-03T11:22:37.888168Z","iopub.status.idle":"2022-12-03T11:22:37.896491Z","shell.execute_reply.started":"2022-12-03T11:22:37.888131Z","shell.execute_reply":"2022-12-03T11:22:37.895128Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def round_off(entry):\n    round_to = [0.0,0.5,0.25,0.75,1]\n    return min(round_to, key=lambda x: abs(x - entry))","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:37.897871Z","iopub.execute_input":"2022-12-03T11:22:37.89887Z","iopub.status.idle":"2022-12-03T11:22:37.90905Z","shell.execute_reply.started":"2022-12-03T11:22:37.898823Z","shell.execute_reply":"2022-12-03T11:22:37.908056Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"submission.score = submission.score.apply(lambda x: round_off(x))","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:37.911456Z","iopub.execute_input":"2022-12-03T11:22:37.911906Z","iopub.status.idle":"2022-12-03T11:22:37.919991Z","shell.execute_reply.started":"2022-12-03T11:22:37.911873Z","shell.execute_reply":"2022-12-03T11:22:37.919097Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:37.922825Z","iopub.execute_input":"2022-12-03T11:22:37.923405Z","iopub.status.idle":"2022-12-03T11:22:37.931113Z","shell.execute_reply.started":"2022-12-03T11:22:37.92337Z","shell.execute_reply":"2022-12-03T11:22:37.930023Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:22:37.932787Z","iopub.execute_input":"2022-12-03T11:22:37.933237Z","iopub.status.idle":"2022-12-03T11:22:37.948835Z","shell.execute_reply.started":"2022-12-03T11:22:37.933205Z","shell.execute_reply":"2022-12-03T11:22:37.947858Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"                  id  score\n0   4112d61851461f60   0.50\n1   5203a36c501f1b7c   0.50\n2   7aa5908a77a7ec24   0.50\n3   09e418c93a776564   0.50\n4   36baf228038e314b   0.25\n5   b892011ab2e2cabc   0.25\n6   1f37ead645e7f0c8   0.50\n7   71a5b6ad068d531f   0.50\n8   16ae4b99d3601e60   0.50\n9   474c874d0c07bd21   0.50\n10  442c114ed5c4e3c9   0.50\n11  b8ae62ea5e1d8bdb   0.50\n12  faaddaf8fcba8a3f   0.50\n13  ae0262c02566d2ce   0.50\n14  a8808e31641e856d   0.50\n15  25c555ca3d5a2092   0.50\n16  b9fdc772bb8fd61c   0.50\n17  d19ef3979396d47e   0.50\n18  7e3aff857f056bf9   0.50\n19  fd83613b7843f5e1   0.50\n20  2a619016908bfa45   0.50\n21  733979d75f59770d   0.50\n22  6546846df17f9800   0.50\n23  3ff0e7a35015be69   0.25\n24  12ca31f018a2e2b9   0.50\n25  03ba802ed4029e4d   0.50\n26  25522ee5411e63e9   0.50\n27  c404f8b378cbb008   0.50\n28  78243984c02a72e4   0.50\n29  de51114bc0faec3e   0.50\n30  26c3c6dc6174b589   0.25\n31  8247ff562ca185cc   0.50\n32  c057aecbba832387   0.50\n33  9f2279ce667b21dc   0.50\n34  b9ea2b06a878df6f   0.50\n35  79795133c30ef097   0.50","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5203a36c501f1b7c</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7aa5908a77a7ec24</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>09e418c93a776564</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36baf228038e314b</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>b892011ab2e2cabc</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1f37ead645e7f0c8</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>71a5b6ad068d531f</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>16ae4b99d3601e60</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>474c874d0c07bd21</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>442c114ed5c4e3c9</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>b8ae62ea5e1d8bdb</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>faaddaf8fcba8a3f</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ae0262c02566d2ce</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>a8808e31641e856d</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>25c555ca3d5a2092</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>b9fdc772bb8fd61c</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>d19ef3979396d47e</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7e3aff857f056bf9</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>fd83613b7843f5e1</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2a619016908bfa45</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>733979d75f59770d</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6546846df17f9800</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>3ff0e7a35015be69</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>12ca31f018a2e2b9</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>03ba802ed4029e4d</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>25522ee5411e63e9</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>c404f8b378cbb008</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>78243984c02a72e4</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>de51114bc0faec3e</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>26c3c6dc6174b589</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>8247ff562ca185cc</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>c057aecbba832387</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>9f2279ce667b21dc</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>b9ea2b06a878df6f</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>79795133c30ef097</td>\n      <td>0.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# test_df = test_df.reset_index(drop=True)\n# te_data = InferDataset(test_df)\n# outputs = trainer.predict(te_data)\n\n# predictions=outputs.predictions.reshape(-1)\n# submission = datasets.Dataset.from_dict({\n#     'id': test_df['id'],\n#     'score': predictions,\n# })","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:18:42.51436Z","iopub.status.idle":"2022-12-03T11:18:42.514874Z","shell.execute_reply.started":"2022-12-03T11:18:42.514598Z","shell.execute_reply":"2022-12-03T11:18:42.51462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission.to_pandas()\n# submission.to_csv('submission.csv' , index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-03T11:18:42.516131Z","iopub.status.idle":"2022-12-03T11:18:42.516815Z","shell.execute_reply.started":"2022-12-03T11:18:42.516595Z","shell.execute_reply":"2022-12-03T11:18:42.51662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}